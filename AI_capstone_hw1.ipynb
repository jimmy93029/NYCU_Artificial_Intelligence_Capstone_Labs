{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jimmy93029/NYCU_Artificial_Intelligence_Capstone_Labs/blob/main/AI_capstone_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RltOQcHze94",
        "outputId": "ac90bdc7-490a-47e0-ff7e-777a3e0e4c2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget -nc https://raw.githubusercontent.com/brpy/colab-pdf/master/colab_pdf.py\n",
        "from colab_pdf import colab_pdf\n",
        "colab_pdf('/content/drive/MyDrive/Colab Notebooks/AI_capstone_hw1.ipynb')"
      ],
      "metadata": {
        "id": "z6gjFKhYluts"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grxN63KFPv6T"
      },
      "outputs": [],
      "source": [
        "!pip install hmmlearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USWfG6VwBeJN"
      },
      "source": [
        "# Pre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1ijkaDjBdjx"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    \"\"\"\n",
        "    Configuration class to centralize all hyperparameters and settings.\n",
        "    \"\"\"\n",
        "    # File paths and names\n",
        "    FOLDER_PATH = \"/content/drive/MyDrive/AI_capstone\"\n",
        "    ORIGINAL_FILE = f\"{FOLDER_PATH}/klines_BTC_9_months.csv\"\n",
        "    FILE = f\"{FOLDER_PATH}/klines_BTC_unmask_9_months.csv\"\n",
        "    VISUALIZATION_FILE = \"btc_targets_analysis.png\"\n",
        "\n",
        "    # Technical indicator parameters\n",
        "    WINDOW_SIZE = 75\n",
        "    VOLATILITY_WINDOW = 24\n",
        "    MA_WINDOWS = [7, 25, 99]\n",
        "    EMA_SHORT = 12\n",
        "    EMA_LONG = 26\n",
        "    MACD_SIGNAL = 9\n",
        "    RSI_PERIOD = 14\n",
        "    BOLLINGER_WINDOW = 20\n",
        "    BOLLINGER_STD = 2\n",
        "\n",
        "    # Outlier detection parameters\n",
        "\n",
        "\n",
        "    # Autoencoder parameters\n",
        "    AUTOENCODER_LAYERS = [64, 32, 16, 32, 64]\n",
        "    REGULARIZATION_FACTOR = 0.001\n",
        "    DROPOUT_RATE = 0.2\n",
        "    EPOCHS = 50\n",
        "    BATCH_SIZE = 32\n",
        "    VALIDATION_SPLIT = 0.4\n",
        "    EARLY_STOPPING_PATIENCE = 5\n",
        "\n",
        "    # Random forest parameters\n",
        "    RF_ESTIMATORS = 100\n",
        "    RF_RANDOM_STATE = 42\n",
        "\n",
        "    # Target classification thresholds\n",
        "    PRICE_MOVEMENT_THRESHOLDS = [-2, -0.5, 0.5, 2]\n",
        "    PRICE_MOVEMENT_LABELS = ['Significant Drop', 'Small Drop', 'Sideways', 'Small Rise', 'Significant Rise']\n",
        "\n",
        "    # Train-test split parameters\n",
        "    TEST_SIZE = 0.2\n",
        "    FORECAST_HORIZON = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bscQHIaiEI7P"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/AI_capstone\")\n",
        "\n",
        "# Now import your script\n",
        "import add_factors\n",
        "from add_factors import prepare_train_test_splits, load_and_clean_data, add_technical_indicators, add_target_variables\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4H4hfNED_HJ"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0iPY-8U6_A3"
      },
      "outputs": [],
      "source": [
        "df_cleaned = load_and_clean_data(Config.ORIGINAL_FILE)\n",
        "df_original = df_cleaned.copy()  # Save original for visualization comparison\n",
        "\n",
        "# Step 2: Add technical indicators\n",
        "df_original = add_technical_indicators(df_original)\n",
        "\n",
        "# Step 3: Add target variables\n",
        "df_original = add_target_variables(df_original)\n",
        "\n",
        "print(\"Available columns:\", df_original.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXEHMKEoLzjS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_outliers(df: pd.DataFrame, outliers_mask: np.ndarray, title):\n",
        "    \"\"\"\n",
        "    Plot time-series data with outliers highlighted using index as x-axis.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        The dataset containing time-series data.\n",
        "    outliers_mask : np.ndarray\n",
        "        Boolean mask where True indicates an outlier.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot normal points in blue\n",
        "    plt.scatter(df.index[~outliers_mask], df[\"close\"][~outliers_mask],\n",
        "                color=\"blue\", alpha=0.7, label=\"Normal Data\")\n",
        "\n",
        "    # Plot outliers in red with a larger marker\n",
        "    plt.scatter(df.index[outliers_mask], df[\"close\"][outliers_mask],\n",
        "                color=\"red\", label=\"Outliers\", marker=\"x\", s=70)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Index (Row Number)\")\n",
        "    plt.ylabel(\"Close Price\")\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.5)  # Add grid for better visibility\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# plot_outliers(df, outliers_mask_zscore)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gteYVTp3EAvG"
      },
      "source": [
        "### Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3eABhx4_V27"
      },
      "outputs": [],
      "source": [
        "AUTOENCODER_LAYERS = [64, 32, 16, 32, 64]\n",
        "ANOMALY_PERCENTILE = 98"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NW2QUgP7_AG4"
      },
      "outputs": [],
      "source": [
        "def Autoencoder(input_dim, layers=AUTOENCODER_LAYERS):\n",
        "\n",
        "    # Build the autoencoder model\n",
        "    autoencoder = keras.Sequential()\n",
        "    # Encoder\n",
        "    autoencoder.add(keras.layers.Dense(\n",
        "        layers[0], activation='relu', input_shape=(input_dim,),\n",
        "        kernel_regularizer=keras.regularizers.l2(Config.REGULARIZATION_FACTOR)))\n",
        "    autoencoder.add(keras.layers.Dropout(Config.DROPOUT_RATE))\n",
        "\n",
        "    for units in layers[1:len(layers)//2 + 1]:\n",
        "        autoencoder.add(keras.layers.Dense(units, activation='relu'))\n",
        "\n",
        "    # Decoder\n",
        "    for units in layers[len(layers)//2 + 1:]:\n",
        "        autoencoder.add(keras.layers.Dense(units, activation='relu'))\n",
        "\n",
        "    autoencoder.add(keras.layers.Dense(input_dim, activation='linear'))\n",
        "    return autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjssm7R1ENoU"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "def detect_outliers(df: pd.DataFrame) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Detect outliers using statistical methods and autoencoder.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame with features\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    numpy.ndarray: Boolean mask where True indicates an outlier\n",
        "    \"\"\"\n",
        "    # Get feature columns (exclude timestamps and targets)\n",
        "    feature_cols = [col for col in df.columns if col not in [\n",
        "        'open_time', 'hour', 'day_of_week', 'is_weekend', 'month',\n",
        "        'target_binary', 'target_pct_change', 'target_abs_change',\n",
        "        'target_multiclass', 'target_log_return', 'target_normalized_change'\n",
        "    ]]\n",
        "\n",
        "    # Autoencoder-based outlier detection\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(df[feature_cols])\n",
        "\n",
        "    autoencoder = Autoencoder(scaled_data.shape[1])\n",
        "\n",
        "    # Compile model\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Train the autoencoder with early stopping\n",
        "    early_stopping = keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=Config.EARLY_STOPPING_PATIENCE,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    autoencoder.fit(\n",
        "        scaled_data, scaled_data,\n",
        "        epochs=Config.EPOCHS,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        validation_split=Config.VALIDATION_SPLIT,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Compute reconstruction error\n",
        "    reconstructed = autoencoder.predict(scaled_data)\n",
        "    reconstruction_error = np.mean(np.abs(scaled_data - reconstructed), axis=1)\n",
        "\n",
        "    # Define anomaly threshold\n",
        "    threshold = np.percentile(reconstruction_error, ANOMALY_PERCENTILE)\n",
        "\n",
        "    # Combine outlier detection methods\n",
        "    outliers_mask = reconstruction_error > threshold\n",
        "    print(f\"Combined methods identified {outliers_mask.sum()} outliers\")\n",
        "\n",
        "    return outliers_mask\n",
        "\n",
        "\n",
        "outliers_mask_autoencoder = detect_outliers(df_original)\n",
        "plot_outliers(df_original, outliers_mask_autoencoder, \"Outliers in Autoencoder\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voFBfxtqLT72"
      },
      "source": [
        "### Isolation forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D90Spyya4cwl"
      },
      "outputs": [],
      "source": [
        "# Select feature columns (excluding non-numeric fields)\n",
        "feature_cols = [col for col in df_original.columns if col not in [\n",
        "    'open_time', 'hour', 'day_of_week', 'is_weekend', 'month',\n",
        "    'target_binary', 'target_pct_change', 'target_abs_change',\n",
        "    'target_multiclass', 'target_log_return', 'target_normalized_change'\n",
        "]]\n",
        "\n",
        "# Extract numerical data\n",
        "X = df_original.loc[:, feature_cols]  # Select only the feature columns\n",
        "\n",
        "# Ensure all data is numeric\n",
        "X = X.apply(pd.to_numeric, errors=\"coerce\").dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-QX-jGGLYwB"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Fit Isolation Forest model\n",
        "iso_forest = IsolationForest(contamination=0.02, random_state=42)\n",
        "outliers = iso_forest.fit_predict(X)  # -1 = outlier, 1 = normal\n",
        "\n",
        "# Create an outlier mask\n",
        "outliers_mask_isolation_forest = outliers == -1\n",
        "\n",
        "# Count detected anomalies\n",
        "print(f\"Isolation Forest detected {outliers_mask_isolation_forest.sum()} outliers.\")\n",
        "\n",
        "# Plot the detected outliers\n",
        "plot_outliers(df_original, outliers_mask_isolation_forest, \"Outliers in Isolation forest\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvhl56q2Yxuh"
      },
      "source": [
        "### Output data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2HE7-UnaF9M"
      },
      "outputs": [],
      "source": [
        "filename = Config.FILE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZ3E8M4YVnzO"
      },
      "outputs": [],
      "source": [
        "total_mask = outliers_mask_autoencoder | outliers_mask_isolation_forest\n",
        "\n",
        "# Ensure total_mask is a pandas Series with the same index\n",
        "total_mask = pd.Series(total_mask, index=df_original.index)\n",
        "\n",
        "# Keep only non-outlier data\n",
        "non_outlier_df = df_original[~total_mask]\n",
        "\n",
        "# Save to CSV\n",
        "non_outlier_df.to_csv(filename, index=False)\n",
        "print(f\"Filtered data saved to {filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXlkFNgO1e4z"
      },
      "source": [
        "# Supervised learning\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8T95ig7CT_T8"
      },
      "outputs": [],
      "source": [
        "# df = df_original\n",
        "\n",
        "df = pd.read_csv(Config.FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjCmegyQZg9B"
      },
      "outputs": [],
      "source": [
        "\n",
        "datas = prepare_train_test_splits(df)\n",
        "\n",
        "print(f\"load data with {Config.FILE}\")\n",
        "print(print(\"Available columns:\", df.columns))\n",
        "\n",
        "feature_cols = datas['feature_columns']\n",
        "\n",
        "X_train = datas['X_train']\n",
        "X_test = datas['X_test']\n",
        "y_train_dict = datas['y_train']\n",
        "y_test_dict = datas['y_test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuWw1DqV1leA"
      },
      "source": [
        "## First part: decision tree vs. random forest vs. decision tree\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfP6P3f7LvLM"
      },
      "source": [
        "### Decision tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN1zmtevLuSu"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import pandas as pd\n",
        "\n",
        "def train_decision_tree_regressor(X_train, y_train, X_test, y_test, feature_cols):\n",
        "    \"\"\"\n",
        "    Train a Decision Tree Regressor to predict continuous values.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_train, y_train : Training dataset\n",
        "    X_test, y_test : Testing dataset\n",
        "    feature_cols : List of feature names\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Model, Predictions, and Feature Importance DataFrame\n",
        "    \"\"\"\n",
        "    # Train Decision Tree Regressor\n",
        "    dt_model = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "    dt_model.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    predictions = dt_model.predict(X_test)\n",
        "\n",
        "    # Analyze feature importance\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_cols,\n",
        "        'importance': dt_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    return dt_model, predictions, feature_importance\n",
        "\n",
        "# Example Usage:\n",
        "# dt_regressor, y_pred, feature_importance_df = train_decision_tree_regressor(X_train, y_train, X_test, y_test, feature_cols)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvW7Ejhd8kH1"
      },
      "source": [
        "### Random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtD02YGY8oEk"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import pandas as pd\n",
        "\n",
        "def train_random_forest_regressor(X_train, y_train, X_test, y_test, feature_cols):\n",
        "    \"\"\"\n",
        "    Train a Random Forest Regressor and analyze feature importance.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_train, y_train : Training dataset\n",
        "    X_test, y_test : Testing dataset\n",
        "    feature_cols : List of feature names\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Model, Predictions, and Feature Importance DataFrame\n",
        "    \"\"\"\n",
        "    # Train Random Forest Regressor\n",
        "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    predictions = rf_model.predict(X_test)\n",
        "\n",
        "    # Analyze feature importance\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_cols,\n",
        "        'importance': rf_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    return rf_model, predictions, feature_importance\n",
        "\n",
        "# Example Usage:\n",
        "# rf_regressor, y_pred, feature_importance_df = train_random_forest_regressor(X_train, y_train, X_test, y_test, feature_cols)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM-IGU6A8aSa"
      },
      "source": [
        "### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1VibDlB542k"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "import pandas as pd\n",
        "\n",
        "def train_xgboost_regressor(X_train, y_train, X_test, y_test, feature_cols):\n",
        "    \"\"\"\n",
        "    Train an XGBoost Regressor and analyze feature importance.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_train, y_train : Training dataset\n",
        "    X_test, y_test : Testing dataset\n",
        "    feature_cols : List of feature names\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Model, Predictions, and Feature Importance DataFrame\n",
        "    \"\"\"\n",
        "    # Train XGBoost Regressor\n",
        "    xgb_model = XGBRegressor(objective=\"reg:squarederror\", eval_metric=\"rmse\")\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    predictions = xgb_model.predict(X_test)\n",
        "\n",
        "    # Analyze feature importance\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_cols,\n",
        "        'importance': xgb_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    return xgb_model, predictions, feature_importance\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr2deT548uHq"
      },
      "source": [
        "### Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZ87Pckz8xMo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_feature_importance(dt_importance, rf_importance, xgb_importance):\n",
        "    \"\"\"\n",
        "    Plot feature importance for Decision Tree, Random Forest, and XGBoost.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    dt_importance : Feature importance from Decision Tree\n",
        "    rf_importance : Feature importance from Random Forest\n",
        "    xgb_importance : Feature importance from XGBoost\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Decision Tree\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.barh(dt_importance['feature'][:10], dt_importance['importance'][:10])\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.title(\"Decision Tree Feature Importance\")\n",
        "\n",
        "    # Random Forest\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.barh(rf_importance['feature'][:10], rf_importance['importance'][:10])\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.title(\"Random Forest Feature Importance\")\n",
        "\n",
        "    # XGBoost\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.barh(xgb_importance['feature'][:10], xgb_importance['importance'][:10])\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.title(\"XGBoost Feature Importance\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B5T2Bxa-5C9"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u4exWs6H-3DB"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Extract only the multiclass labels for classification\n",
        "y_train = y_train_dict['target_pct_change']\n",
        "y_test = y_test_dict['target_pct_change']\n",
        "\n",
        "# Train Models\n",
        "dt_model, dt_preds, dt_importance = train_decision_tree_regressor(X_train, y_train, X_test, y_test, feature_cols)\n",
        "\n",
        "rf_model, rf_preds, rf_importance = train_random_forest_regressor(X_train, y_train, X_test, y_test, feature_cols)\n",
        "\n",
        "xgb_model, xgb_preds, xgb_importance = train_xgboost_regressor(X_train, y_train, X_test, y_test, feature_cols)\n",
        "\n",
        "\n",
        "# Compare Feature Importance\n",
        "plot_feature_importance(dt_importance, rf_importance, xgb_importance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zznZ3I0JJLu_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Compute MAE\n",
        "dt_mae = mean_absolute_error(y_test, dt_preds)\n",
        "rf_mae = mean_absolute_error(y_test, rf_preds)\n",
        "xgb_mae = mean_absolute_error(y_test, xgb_preds)\n",
        "\n",
        "# Compute MSE\n",
        "dt_mse = mean_squared_error(y_test, dt_preds)\n",
        "rf_mse = mean_squared_error(y_test, rf_preds)\n",
        "xgb_mse = mean_squared_error(y_test, xgb_preds)\n",
        "\n",
        "# Compute RMSE\n",
        "dt_rmse = dt_mse ** 0.5\n",
        "rf_rmse = rf_mse ** 0.5\n",
        "xgb_rmse = xgb_mse ** 0.5\n",
        "\n",
        "# Compute RÂ² Score\n",
        "dt_r2 = r2_score(y_test, dt_preds)\n",
        "rf_r2 = r2_score(y_test, rf_preds)\n",
        "xgb_r2 = r2_score(y_test, xgb_preds)\n",
        "\n",
        "# Print results\n",
        "print(f\"Decision Tree: MAE={dt_mae:.4f}, RMSE={dt_rmse:.4f}, RÂ²={dt_r2:.4f}\")\n",
        "print(f\"Random Forest: MAE={rf_mae:.4f}, RMSE={rf_rmse:.4f}, RÂ²={rf_r2:.4f}\")\n",
        "print(f\"XGBoost: MAE={xgb_mae:.4f}, RMSE={xgb_rmse:.4f}, RÂ²={xgb_r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecZQjLk51tp1"
      },
      "source": [
        "## Second part: LSTM vs. Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cBWmiLnePiMN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load preprocessed data splits\n",
        "datas = prepare_train_test_splits(df)\n",
        "\n",
        "X_train, X_test = datas['X_train'], datas['X_test']\n",
        "y_train, y_test = datas['y_train']['target_pct_change'], datas['y_test']['target_pct_change']\n",
        "\n",
        "# Reshape data for LSTM & Transformer: (samples, time_steps, features)\n",
        "time_steps = 120  # Number of previous time steps used for prediction\n",
        "\n",
        "def reshape_to_sequences(X, y, time_steps):\n",
        "    \"\"\"\n",
        "    Reshape data into sequences for time series forecasting.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : np.array\n",
        "        Feature data\n",
        "    y : np.array\n",
        "        Target variable\n",
        "    time_steps : int\n",
        "        Number of past time steps to use as input\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X_seq, y_seq : np.array\n",
        "        Reshaped feature and target arrays\n",
        "    \"\"\"\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        X_seq.append(X[i:i+time_steps])\n",
        "        y_seq.append(y[i+time_steps])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "X_train_seq, y_train_seq = reshape_to_sequences(X_train, y_train, time_steps)\n",
        "X_test_seq, y_test_seq = reshape_to_sequences(X_test, y_test, time_steps)\n",
        "\n",
        "print(f\"Reshaped X_train: {X_train_seq.shape}, Reshaped X_test: {X_test_seq.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4d9b3mkPj3Y"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6u1QA9aza1Xs"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "\n",
        "\n",
        "def build_lstm_model(input_shape):\n",
        "    \"\"\"\n",
        "    Build and compile an LSTM model for time series forecasting.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    input_shape : tuple\n",
        "        The shape of the input data (time_steps, features).\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    model : keras.Model\n",
        "        Compiled LSTM model.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        LSTM(64, return_sequences=True, input_shape=input_shape),\n",
        "        Dropout(0.2),\n",
        "        LSTM(64, return_sequences=False),\n",
        "        Dropout(0.2),\n",
        "        Dense(32, activation=\"relu\"),\n",
        "        Dense(1)  # Predict next time step\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8OwebBxwPmEC"
      },
      "outputs": [],
      "source": [
        "# âœ… Ensure input shape is correctly passed\n",
        "lstm_model = build_lstm_model((X_train.shape[0], X_train.shape[1]))\n",
        "steps = 100\n",
        "\n",
        "# Define Early Stopping Callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',   # Monitor validation loss\n",
        "    patience=5,           # Wait 5 epochs for improvement\n",
        "    min_delta=0.0001,     # Minimum change to qualify as improvement\n",
        "    mode='min',           # Stop when val_loss is minimized\n",
        "    restore_best_weights=True  # Restore best weights when stopping\n",
        ")\n",
        "\n",
        "# Model Checkpointing\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=\"best_lstm_model.h5\",\n",
        "    monitor=\"val_loss\",\n",
        "    save_best_only=True,  # Save only the best model\n",
        "    mode=\"min\",\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train LSTM with Early Stopping and Checkpointing\n",
        "history = lstm_model.fit(\n",
        "    X_train_seq, y_train_seq,\n",
        "    epochs=steps,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[checkpoint_callback, early_stopping]  # Use both Early Stopping & Checkpoint\n",
        ")\n",
        "\n",
        "# Predict with LSTM\n",
        "lstm_preds = lstm_model.predict(X_test_seq)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7QhY3a7Pvx4"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QtAsGaeuPyHe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import multiprocessing\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True  # Optimize GPU training\n",
        "\n",
        "# Convert NumPy arrays to PyTorch tensors FIRST\n",
        "X_train_torch = torch.from_numpy(X_train_seq).float()\n",
        "y_train_torch = torch.from_numpy(y_train_seq).float().unsqueeze(-1)\n",
        "\n",
        "X_test_torch = torch.from_numpy(X_test_seq).float()\n",
        "y_test_torch = torch.from_numpy(y_test_seq).float().unsqueeze(-1)\n",
        "\n",
        "# Use pin_memory() AFTER converting to tensor\n",
        "X_train_torch = X_train_torch.pin_memory()\n",
        "y_train_torch = y_train_torch.pin_memory()\n",
        "X_test_torch = X_test_torch.pin_memory()\n",
        "y_test_torch = y_test_torch.pin_memory()\n",
        "\n",
        "# Move data to GPU only during training\n",
        "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
        "test_dataset = TensorDataset(X_test_torch, y_test_torch)\n",
        "\n",
        "# Dynamically set num_workers for best performance\n",
        "num_workers = min(4, multiprocessing.cpu_count() - 1)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=num_workers, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3zCCO2_Pb6Su"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_model(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    for X_batch, y_batch in val_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        total_loss += loss.item()\n",
        "    model.train()  # Set model back to training mode\n",
        "    return total_loss / len(val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QKG0efmT1vQ-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "class TransformerTimeSeries(nn.Module):\n",
        "    \"\"\"\n",
        "    Improved Transformer model for time series forecasting.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, embed_dim=128, num_heads=8, ff_dim=256, num_layers=6, dropout=0.1):  # Dropout fixed\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
        "        self.positional_encoding = self.get_sinusoidal_encoding(512, embed_dim)  # Fixed Positional Encoding\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout)\n",
        "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(embed_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) + self.positional_encoding[:x.shape[1], :].unsqueeze(0).to(x.device)\n",
        "        x = x.permute(1, 0, 2)  # (seq_len, batch_size, embed_dim)\n",
        "        x = self.encoder(x)\n",
        "        x = x.permute(1, 0, 2)  # Convert back\n",
        "        return self.fc(x[:, -1, :])  # Predict next time step\n",
        "\n",
        "    def get_sinusoidal_encoding(self, seq_len, d_model):\n",
        "        position = torch.arange(seq_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        return pe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QX153TxZ1d5Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Training Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 32  # Increase batch size\n",
        "lr = 0.0005  # Reduce learning rate\n",
        "dropout = 0.1  # Fix dropout\n",
        "transformer_model = TransformerTimeSeries(input_dim=X_train_seq.shape[2], dropout=dropout).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(transformer_model.parameters(), lr=lr)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "patience = 10\n",
        "patience_counter = 0\n",
        "steps = 100\n",
        "\n",
        "for epoch in range(steps):\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # âœ… Move batches to GPU\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with torch.amp.autocast(device_type=\"cuda\"):  # âœ… Updated for new PyTorch version\n",
        "            outputs = transformer_model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), max_norm=1.0)  # Gradient Clipping\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "\n",
        "    val_loss = evaluate_model(transformer_model, test_loader, criterion)\n",
        "\n",
        "    print(f\"Epoch {epoch}: Train Loss {loss.item():.4f}, Val Loss {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(transformer_model.state_dict(), \"transformer_best.pth\")\n",
        "        print(f\"Model saved at epoch {epoch} (New Best Val Loss: {val_loss:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UD7R-H-gCR3Y"
      },
      "outputs": [],
      "source": [
        "# Move model to GPU\n",
        "transformer_model.to(device)\n",
        "\n",
        "# Move data to GPU (before passing it to the model)\n",
        "with torch.no_grad():\n",
        "    transformer_preds = transformer_model(X_test_torch.to(device))  # âœ… Move data to the same device\n",
        "    transformer_preds = transformer_preds.cpu().numpy()  # âœ… Move back to CPU if needed for further processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flQjL9VrP2d4"
      },
      "source": [
        "### Evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "texloXx2P9I5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Compute errors\n",
        "lstm_mae = mean_absolute_error(y_test_seq, lstm_preds)\n",
        "lstm_mse = mean_squared_error(y_test_seq, lstm_preds)\n",
        "\n",
        "transformer_mae = mean_absolute_error(y_test_seq, transformer_preds)\n",
        "transformer_mse = mean_squared_error(y_test_seq, transformer_preds)\n",
        "\n",
        "print(f\"LSTM MAE: {lstm_mae:.4f}, MSE: {lstm_mse:.4f}\")\n",
        "print(f\"Transformer MAE: {transformer_mae:.4f}, MSE: {transformer_mse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOJKE3XWP-Fs"
      },
      "source": [
        "### Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tuPvthWaQAGv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot LSTM predictions\n",
        "plt.plot(y_test_seq, label=\"Actual Price\", alpha=0.6)\n",
        "plt.plot(lstm_preds, label=\"LSTM Prediction\", alpha=0.8)\n",
        "plt.title(\"LSTM Time-Series Forecasting\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot Transformer predictions\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_test_seq, label=\"Actual Price\", alpha=0.6)\n",
        "plt.plot(transformer_preds, label=\"Transformer Prediction\", alpha=0.8)\n",
        "plt.title(\"Transformer Time-Series Forecasting\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qqG8Fmw19sY0"
      },
      "outputs": [],
      "source": [
        "!cp /content/best_lstm_model.h5 /content/drive/MyDrive/AI_capstone\n",
        "!cp /content/transformer_best.pth /content/drive/MyDrive/AI_capstone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGuilZNc1587"
      },
      "source": [
        "# Unsupervised learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opDvWexW19qy"
      },
      "source": [
        "## density estimation : GMM vs.HMM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4FFEMTyiSrw4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from hmmlearn.hmm import GaussianHMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OhnXso8tNYbq"
      },
      "outputs": [],
      "source": [
        "# Load the cleaned dataset\n",
        "df = pd.read_csv(Config.FILE)\n",
        "\n",
        "# Prepare train/test splits\n",
        "datas = prepare_train_test_splits(df)\n",
        "\n",
        "# Extract transformed feature matrix\n",
        "X_train, X_test = datas['X_train'], datas['X_test']\n",
        "y_train, y_test = datas['y_train']['target_pct_change'], datas['y_test']['target_pct_change']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1kz6wavKs6n"
      },
      "source": [
        "### Gaussian mixture model (GMM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e4KzsEH519De"
      },
      "outputs": [],
      "source": [
        "# âœ… Fit GMM on training data\n",
        "gmm = GaussianMixture(n_components=3, covariance_type=\"full\", random_state=42)\n",
        "gmm.fit(X_train)\n",
        "\n",
        "# âœ… Predict clusters for both training and test sets\n",
        "train_clusters = gmm.predict(X_train)\n",
        "test_clusters = gmm.predict(X_test)\n",
        "\n",
        "# âœ… Assign clusters to DataFrame\n",
        "df.loc[datas['train_dates'].index, \"GMM_Cluster\"] = train_clusters\n",
        "df.loc[datas['test_dates'].index, \"GMM_Cluster\"] = test_clusters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DAXb-FRK93j"
      },
      "source": [
        "### Hidden Markov model (HMM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mkSKgGnWLDii"
      },
      "outputs": [],
      "source": [
        "from hmmlearn.hmm import GaussianHMM\n",
        "\n",
        "# âœ… Train HMM with 3 hidden states (e.g., bull, bear, neutral)\n",
        "hmm = GaussianHMM(n_components=3, covariance_type=\"diag\", n_iter=1000, random_state=42)\n",
        "hmm.fit(X_train)\n",
        "\n",
        "# âœ… Predict hidden states for train & test data\n",
        "train_states = hmm.predict(X_train)\n",
        "test_states = hmm.predict(X_test)\n",
        "\n",
        "# âœ… Store hidden states in DataFrame\n",
        "df.loc[datas['train_dates'].index, \"HMM_State\"] = train_states\n",
        "df.loc[datas['test_dates'].index, \"HMM_State\"] = test_states\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLKTy6gaaL5z"
      },
      "source": [
        "### Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FXnhtR1gSd9w"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(df.index, df[\"close\"], c=df[\"GMM_Cluster\"], cmap=\"viridis\", alpha=0.7)\n",
        "plt.title(\"GMM Clustering on Price Data\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.colorbar(label=\"Cluster Label\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tbG_bxxNLlk9"
      },
      "outputs": [],
      "source": [
        "# Plot market regimes over price data\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.scatter(df[\"open_time\"], df[\"close\"], c=df[\"HMM_State\"], cmap=\"coolwarm\", label=\"Market States\")\n",
        "plt.colorbar(label=\"Market Regime (HMM States)\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Closing Price\")\n",
        "plt.title(\"HMM-Inferred Market Regimes with Technical Indicators\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6feBgaf6ZTeT"
      },
      "source": [
        "### Compare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bAIdCDxvORU7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Select only numeric columns (excluding datetime & categorical data)\n",
        "numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
        "\n",
        "# Group by GMM clusters and compute average values\n",
        "gmm_cluster_averages = df.groupby(\"GMM_Cluster\")[numeric_cols].mean()\n",
        "print(\"\\nðŸ”¹ GMM Cluster Averages:\")\n",
        "print(gmm_cluster_averages)\n",
        "\n",
        "# Group by HMM states and compute average values\n",
        "hmm_cluster_averages = df.groupby(\"HMM_State\")[numeric_cols].mean()\n",
        "print(\"\\nðŸ”¹ HMM Market Regime Averages:\")\n",
        "print(hmm_cluster_averages)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOviaoLB+iD/U1XZK4T4o1k",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}